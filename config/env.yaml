# hyper_param.yml - Configuration file for DQN hyperparameters

gridworld_basic:
  env_id: 'GridWorld'
  env_make_param:
    n_width: 8
    n_height: 6
    default_reward: -0.1
    start: [0, 0]
    ends: [[7, 5]]
    types: [[3, 3, 1], [4, 3, 1], [5, 3, 1]]  # obstacles
    rewards: [[7, 5, 100], [2, 4, -10], [6, 2, -10]]
  replay_memory_size: 10000
  mini_batch_size: 32
  epsilon_init: 1.0
  epsilon_decay: 0.995
  epsilon_min: 0.01
  learning_rate_a: 0.001
  discount_factor_g: 0.99
  network_sync_rate: 100
  stop_on_reward: 80
  fc1_nodes: 128

gridworld_advanced:
  env_id: 'GridWorld'
  env_make_param:
    n_width: 12
    n_height: 8
    default_reward: -0.1
    start: [0, 0]
    ends: [[11, 7]]
    types: [[4, 2, 1], [4, 3, 1], [4, 4, 1], [4, 5, 1], [7, 1, 1], [7, 2, 1], [7, 3, 1], [7, 4, 1]]
    rewards: [[11, 7, 100], [3, 6, -20], [8, 5, -20], [6, 3, 5]]
  replay_memory_size: 20000
  mini_batch_size: 64
  epsilon_init: 1.0
  epsilon_decay: 0.999
  epsilon_min: 0.01
  learning_rate_a: 0.0005
  discount_factor_g: 0.99
  network_sync_rate: 200
  stop_on_reward: 90
  fc1_nodes: 256

gridworld_maze:
  env_id: 'GridWorld'
  env_make_param:
    n_width: 15
    n_height: 10
    default_reward: -0.1
    start: [0, 0]
    ends: [[14, 9]]
    # Maze-like environment
    types: [
      [2, 2, 1], [2, 3, 1], [2, 4, 1], [2, 5, 1], [2, 6, 1],
      [4, 1, 1], [4, 2, 1], [4, 3, 1], [4, 4, 1], [4, 7, 1], [4, 8, 1],
      [6, 3, 1], [6, 4, 1], [6, 5, 1], [6, 6, 1], [6, 7, 1],
      [8, 1, 1], [8, 2, 1], [8, 3, 1], [8, 5, 1], [8, 6, 1],
      [10, 2, 1], [10, 3, 1], [10, 4, 1], [10, 5, 1], [10, 6, 1], [10, 7, 1],
      [12, 1, 1], [12, 2, 1], [12, 4, 1], [12, 5, 1], [12, 6, 1], [12, 7, 1], [12, 8, 1]
    ]
    rewards: [[14, 9, 200], [5, 8, -30], [9, 4, -30], [11, 6, -30], [7, 7, 10]]
  replay_memory_size: 25000
  mini_batch_size: 64
  epsilon_init: 1.0
  epsilon_decay: 0.9995
  epsilon_min: 0.01
  learning_rate_a: 0.0005
  discount_factor_g: 0.99
  network_sync_rate: 150
  stop_on_reward: 150
  fc1_nodes: 256

cliff_walking:
  env_id: 'GridWorld'
  env_make_param:
    n_width: 12
    n_height: 4
    default_reward: -1
    start: [0, 0]
    ends: [[11, 0]]
    types: []  # No obstacles
    # Cliff punishment
    rewards: [
      [11, 0, 100],  # Goal reward
      [1, 0, -100], [2, 0, -100], [3, 0, -100], [4, 0, -100], [5, 0, -100],
      [6, 0, -100], [7, 0, -100], [8, 0, -100], [9, 0, -100], [10, 0, -100]
    ]
  replay_memory_size: 15000
  mini_batch_size: 32
  epsilon_init: 1.0
  epsilon_decay: 0.99
  epsilon_min: 0.05
  learning_rate_a: 0.001
  discount_factor_g: 0.99
  network_sync_rate: 100
  stop_on_reward: 50
  fc1_nodes: 128

# For OpenAI Gym environments
cartpole:
  env_id: 'CartPole-v1'
  env_make_param: {}
  replay_memory_size: 50000
  mini_batch_size: 32
  epsilon_init: 1.0
  epsilon_decay: 0.995
  epsilon_min: 0.01
  learning_rate_a: 0.001
  discount_factor_g: 0.99
  network_sync_rate: 100
  stop_on_reward: 195
  fc1_nodes: 128

lunarlander:
  env_id: 'LunarLander-v2'
  env_make_param: {}
  replay_memory_size: 100000
  mini_batch_size: 64
  epsilon_init: 1.0
  epsilon_decay: 0.999
  epsilon_min: 0.01
  learning_rate_a: 0.0005
  discount_factor_g: 0.99
  network_sync_rate: 200
  stop_on_reward: 200
  fc1_nodes: 256