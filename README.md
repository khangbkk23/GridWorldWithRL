# ðŸ§  DQN Agent for GridWorld & OpenAI Gym Environments

A PyTorch implementation of Deep Q-Learning (DQN) for various GridWorld scenarios and classic OpenAI Gym environments.
This project demonstrates how to train a DQN agent using both custom GridWorld environments and standard OpenAI Gym environments like CartPole and LunarLander.

---

## ðŸ“ Folder Structure

```
.
â”œâ”€â”€ config/                    # YAML config files for environment setup
â”‚   â””â”€â”€ env.yaml
â”œâ”€â”€ models/                   # Saved trained models
â”‚   â”œâ”€â”€ dqn_gridworld_basic.pth
â”‚   â””â”€â”€ dqn_gridworld_maze.pth
â”œâ”€â”€ reinforcement/            # Core reinforcement learning modules
â”‚   â”œâ”€â”€ agent.py
â”‚   â”œâ”€â”€ dqn.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ results/                  # Output folder for plots, training results, etc.
â”œâ”€â”€ test/                     # Test script for basic functionality
â”‚   â””â”€â”€ test.py
â”œâ”€â”€ gridworld.py              # Environment definition (4x4 GridWorld)
â”œâ”€â”€ main.py                   # Entry point for training and testing
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 # Project documentation
â””â”€â”€ .gitignore
```

---

## âš™ï¸ Installation

```bash
# Create a virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install required packages
pip install -r requirements.txt
```

---

## ðŸš€ Training the Agent

To train the agent in a specific environment (e.g., basic or maze):

### Train on GridWorld Environments

```bash
python main.py --config gridworld_basic --episodes 1500 --visualize
```
```bash
python main.py --config gridworld_advanced --episodes 2000 --visualize
```
```bash
python main.py --config gridworld_maze --episodes 3000 --visualize
```
```bash
python main.py --config cliff_walking --episodes 1500 --visualize
```
```bash
python main.py --config windy_gridworld --episodes 1500 --visualize

```
### Train on OpenAI Gym Environment

```bash
python main.py --config cartpole --episodes 500
```
```bash
python main.py --config lunarlander --episodes 1000
```

- `--episodes`: Number of episodes to train
- `--visualize`: (Optional) Turn on real-time training visualization

---

## ðŸ“‚ Supported Environments

The following environments are available via `--config`:

- `gridworld_basic`
- `gridworld_advanced`
- `gridworld_maze`
- `cliff_walking`
- `windy_gridworld`
- `cartpole`
- `lunarlander`

## ðŸ§ª Testing the Agent

Run all the built-in tests to verify the setup:

```bash
python test/test.py
```

This checks:
- GridWorld environment functionality
- DQN network architecture
- Agent logic
- Training loop

---

## ðŸ“Š Visualizing Results

After training with `--visualize`, the `results/` folder will contain:
- Training scores (plot image)
- Reward history
- Saved model weights in `models/`

You can also modify the plotting functions inside `utils.py` to customize the visuals.

---

## ðŸ›  Configuration

Edit `config/env.yaml` to modify:
- Grid size
- Reward structure
- Training hyperparameters

---

## âœ… Requirements

Make sure you're using:
- Python 3.12.3
- PyTorch >= 2.7.1 (CPU-only is fine)

---

## ðŸ“Œ Notes

- The project uses **one-hot encoding** for GridWorld states.
- Both Îµ-greedy strategy and target network synchronization are implemented.
- Modular design allows easy extension to other environments.